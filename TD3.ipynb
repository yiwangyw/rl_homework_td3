{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLRdee6I7tw3"
      },
      "source": [
        "This is an example of running a deep reinforcement learning algorithm. This file is created for a course at Kyushu Institute of Technology entitiled \"Brain-Inspired Learning Theory B\".\n",
        "\n",
        "In the following, we run Twin Delayed Deep Deterministic Policy Gradient (TD3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0lwnQCWhElA"
      },
      "source": [
        "# Neural Network Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDbjVC6oalVN"
      },
      "source": [
        "We define classes for the actor and critic networks. In TD3, we prepare two neural networks for the critic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQj4wEKr7jWv"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f3 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "        self.f4 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f5 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f6 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def predict(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        x2 = self.f4(tf.concat([state, action], axis=1))\n",
        "        x2 = self.f5(x2)\n",
        "        q2 = self.f6(x2)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        return q1\n",
        "\n",
        "class Actor(tf.keras.Model):\n",
        "  def __init__(self, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "    self.mu =  tf.keras.layers.Dense(action_dim, activation='tanh')\n",
        "\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def predict(self, state):\n",
        "    x = self.f1(state)\n",
        "    x = self.f2(x)\n",
        "    a = self.max_action * self.mu(x)\n",
        "\n",
        "    return a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sas actor and critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class SAC_Actor(tf.keras.Model):\n",
        "    def __init__(self, action_dim, max_action):\n",
        "        super(SAC_Actor, self).__init__()\n",
        "        self.l1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.l2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.mean = tf.keras.layers.Dense(action_dim)\n",
        "        self.log_std = tf.keras.layers.Dense(action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.l1(state)\n",
        "        x = self.l2(x)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x)\n",
        "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.call(state)\n",
        "        std = tf.exp(log_std)\n",
        "        normal = tf.random.normal(shape=mean.shape)\n",
        "        action = mean + std * normal\n",
        "        action = tf.tanh(action) * self.max_action\n",
        "        log_prob = tf.reduce_sum(-0.5 * ((normal - mean) / (std + 1e-6)) ** 2 - 0.5 * np.log(2 * np.pi) - log_std, axis=1, keepdims=True)\n",
        "        log_prob -= tf.reduce_sum(tf.math.log(1 - action ** 2 + 1e-6), axis=1, keepdims=True)\n",
        "        return action, log_prob\n",
        "\n",
        "class SAC_Critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(SAC_Critic, self).__init__()\n",
        "        self.l1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.l2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.q = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, state_action):\n",
        "        x = self.l1(state_action)\n",
        "        x = self.l2(x)\n",
        "        q = self.q(x)\n",
        "        return q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pFfw7tE7sBn"
      },
      "source": [
        "We then define the class for TD3. The update of the actor and critic is defined in \"def train()\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kC1xWehEaip1"
      },
      "outputs": [],
      "source": [
        "class TD3(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            state_dim,\n",
        "            action_dim,\n",
        "            max_action,\n",
        "            discount=0.99,\n",
        "            tau=0.005,\n",
        "            policy_noise=0.2,\n",
        "            noise_clip=0.5,\n",
        "            policy_freq=2\n",
        "    ):\n",
        "\n",
        "        self.actor = Actor(action_dim, max_action)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "\n",
        "        self.critic = Critic()\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(3e-4)\n",
        "\n",
        "        self.actor.compile(optimizer=self.actor_optimizer)\n",
        "        self.critic.compile(optimizer=self.critic_optimizer)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        return tf.squeeze(self.actor.predict(state), axis=1)\n",
        "\n",
        "    def update_actor_target_network(self):\n",
        "        actor_weights = self.actor.get_weights()\n",
        "        actor_target_weights = self.actor_target.get_weights()\n",
        "        for i in range(len(actor_weights)):\n",
        "            actor_target_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * actor_target_weights[i]\n",
        "\n",
        "        self.actor_target.set_weights(actor_target_weights)\n",
        "\n",
        "    def update_critic_target_network(self):\n",
        "        critic_weights = self.critic.get_weights()\n",
        "        critic_target_weights = self.critic_target.get_weights()\n",
        "        for i in range(len(critic_weights)):\n",
        "            critic_target_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * critic_target_weights[i]\n",
        "\n",
        "        self.critic_target.set_weights(critic_target_weights)\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=100):\n",
        "        self.total_it += 1\n",
        "\n",
        "        # Sample replay buffer\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with tf.GradientTape() as tape_critic:\n",
        "            target_actions = self.actor_target.predict(next_state)\n",
        "\n",
        "            noise = tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=self.policy_noise),\n",
        "                                     -self.noise_clip, self.noise_clip)\n",
        "\n",
        "            next_action = (tf.clip_by_value(target_actions + noise, -self.max_action, self.max_action))\n",
        "\n",
        "            target_Q1, target_Q2 = self.critic_target.predict(next_state, next_action)\n",
        "            target_Q = tf.math.minimum(target_Q1, target_Q2)\n",
        "            target_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "            current_Q1, current_Q2 = self.critic.predict(state, action)\n",
        "\n",
        "            critic_loss = tf.keras.losses.MSE(current_Q1, tf.stop_gradient(target_Q)) \\\n",
        "                          + tf.keras.losses.MSE(current_Q2, tf.stop_gradient(target_Q))\n",
        "\n",
        "            grads_critic = tape_critic.gradient(critic_loss, self.critic.trainable_variables)\n",
        "\n",
        "        self.critic_optimizer.apply_gradients(zip(grads_critic, self.critic.trainable_variables))\n",
        "\n",
        "        if self.total_it % self.policy_freq == 0:\n",
        "            with tf.GradientTape() as tape_actor:\n",
        "                actor_loss = - tf.math.reduce_mean(self.critic.Q1(state, self.actor.predict(state)))\n",
        "\n",
        "\n",
        "            grads_actor = tape_actor.gradient(actor_loss, self.actor.trainable_variables)\n",
        "            self.actor_optimizer.apply_gradients(zip(grads_actor, self.actor.trainable_variables))\n",
        "\n",
        "            self.update_actor_target_network()\n",
        "            self.update_critic_target_network()\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, iter, seed, env_name, foldername='./model/td3'  ):\n",
        "        try:\n",
        "            import pathlib\n",
        "            pathlib.Path(foldername).mkdir(parents=True, exist_ok=True)\n",
        "            self.actor.save_weights(\n",
        "                       foldername + '/td3_actor_'+ env_name +\n",
        "                       '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n",
        "\n",
        "            print('models is saved for iteration', iter )\n",
        "\n",
        "        except:\n",
        "            print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
        "\n",
        "    def load_model(self, iter, seed, env_name, foldername='model/td3'  ):\n",
        "        self.actor.load_weights(foldername + '/td3_actor_' + env_name +\n",
        "            '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAC_Actor(tf.keras.Model):\n",
        "    def __init__(self, action_dim, max_action):\n",
        "        super(SAC_Actor, self).__init__()\n",
        "        self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.mean = tf.keras.layers.Dense(action_dim)\n",
        "        self.log_std = tf.keras.layers.Dense(action_dim)\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.f1(state)\n",
        "        x = self.f2(x)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std(x)\n",
        "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.call(state)\n",
        "        std = tf.exp(log_std)\n",
        "        normal = tf.random.normal(shape=mean.shape)\n",
        "        action = tf.tanh(mean + std * normal) * self.max_action\n",
        "        log_prob = -0.5 * (normal ** 2 + 2 * log_std + np.log(2 * np.pi))\n",
        "        log_prob = tf.reduce_sum(log_prob, axis=1, keepdims=True)\n",
        "        return action, log_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAC_Critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(SAC_Critic, self).__init__()\n",
        "        self.f1 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f3 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "        self.f4 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f5 = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.f6 = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def predict(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        x2 = self.f4(tf.concat([state, action], axis=1))\n",
        "        x2 = self.f5(x2)\n",
        "        q2 = self.f6(x2)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        x = self.f1(tf.concat([state, action], axis=1))\n",
        "        x = self.f2(x)\n",
        "        q1 = self.f3(x)\n",
        "\n",
        "        return q1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAC(object):\n",
        "    def __init__(\n",
        "            self,\n",
        "            state_dim,\n",
        "            action_dim,\n",
        "            max_action,\n",
        "            discount=0.99,\n",
        "            tau=0.005,\n",
        "            alpha=0.2\n",
        "    ):\n",
        "        self.actor = SAC_Actor(action_dim, max_action)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = tf.keras.optimizers.legacy.Adam(3e-4)\n",
        "\n",
        "        self.critic1 = SAC_Critic()\n",
        "        self.critic2 = SAC_Critic()\n",
        "        self.critic1_target = copy.deepcopy(self.critic1)\n",
        "        self.critic2_target = copy.deepcopy(self.critic2)\n",
        "        self.critic_optimizer = tf.keras.optimizers.legacy.Adam(3e-4)\n",
        "\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha  # Temperature parameter for entropy\n",
        "\n",
        "        self.total_it = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        action, _ = self.actor.sample(state)\n",
        "        return tf.squeeze(action, axis=0)\n",
        "\n",
        "    def update_actor_target_network(self):\n",
        "        actor_weights = self.actor.get_weights()\n",
        "        actor_target_weights = self.actor_target.get_weights()\n",
        "        for i in range(len(actor_weights)):\n",
        "            actor_target_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * actor_target_weights[i]\n",
        "        self.actor_target.set_weights(actor_target_weights)\n",
        "\n",
        "    def update_critic_target_network(self):\n",
        "        critic1_weights = self.critic1.get_weights()\n",
        "        critic2_weights = self.critic2.get_weights()\n",
        "        critic1_target_weights = self.critic1_target.get_weights()\n",
        "        critic2_target_weights = self.critic2_target.get_weights()\n",
        "        for i in range(len(critic1_weights)):\n",
        "            critic1_target_weights[i] = self.tau * critic1_weights[i] + (1 - self.tau) * critic1_target_weights[i]\n",
        "            critic2_target_weights[i] = self.tau * critic2_weights[i] + (1 - self.tau) * critic2_target_weights[i]\n",
        "        self.critic1_target.set_weights(critic1_target_weights)\n",
        "        self.critic2_target.set_weights(critic2_target_weights)\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=100):\n",
        "        self.total_it += 1\n",
        "\n",
        "        # Sample replay buffer\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            next_action, next_log_prob = self.actor_target.sample(next_state)\n",
        "            target_Q1 = self.critic1_target.predict(next_state, next_action)\n",
        "            target_Q2 = self.critic2_target.predict(next_state, next_action)\n",
        "            target_Q = tf.minimum(target_Q1, target_Q2) - self.alpha * next_log_prob\n",
        "            target_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "            current_Q1 = self.critic1.predict(state, action)\n",
        "            current_Q2 = self.critic2.predict(state, action)\n",
        "            critic_loss1 = tf.keras.losses.MSE(current_Q1, tf.stop_gradient(target_Q))\n",
        "            critic_loss2 = tf.keras.losses.MSE(current_Q2, tf.stop_gradient(target_Q))\n",
        "\n",
        "        critic_grads1 = tape.gradient(critic_loss1, self.critic1.trainable_variables)\n",
        "        critic_grads2 = tape.gradient(critic_loss2, self.critic2.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads1, self.critic1.trainable_variables))\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads2, self.critic2.trainable_variables))\n",
        "\n",
        "        if self.total_it % 2 == 0:\n",
        "            with tf.GradientTape() as tape_actor:\n",
        "                actions, log_prob = self.actor.sample(state)\n",
        "                Q1 = self.critic1.Q1(state, actions)\n",
        "                Q2 = self.critic2.Q1(state, actions)\n",
        "                actor_loss = tf.reduce_mean(self.alpha * log_prob - tf.minimum(Q1, Q2))\n",
        "\n",
        "            actor_grads = tape_actor.gradient(actor_loss, self.actor.trainable_variables)\n",
        "            self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "\n",
        "            self.update_actor_target_network()\n",
        "            self.update_critic_target_network()\n",
        "\n",
        "    def save_model(self, iter, seed, env_name, foldername='./model/sac'):\n",
        "        try:\n",
        "            import pathlib\n",
        "            pathlib.Path(foldername).mkdir(parents=True, exist_ok=True)\n",
        "            self.actor.save_weights(\n",
        "                foldername + '/sac_actor_' + env_name +\n",
        "                '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n",
        "            print('models is saved for iteration', iter)\n",
        "        except:\n",
        "            print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
        "\n",
        "    def load_model(self, iter, seed, env_name, foldername='model/sac'):\n",
        "        self.actor.load_weights(foldername + '/sac_actor_' + env_name +\n",
        "                                '_seed' + str(seed) + '_iter' + str(iter) + '.tf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWAgIHhdhcYD"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm1Og8m4cGDA"
      },
      "source": [
        "The samples collected through trials and errors are stored in the replay buffer. \"def sample()\" is a function that randomly samples a batch of samples from the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10_yuayicCbp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        self.state = np.zeros((max_size, state_dim))\n",
        "        self.action = np.zeros((max_size, action_dim))\n",
        "        self.next_state = np.zeros((max_size, state_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            tf.convert_to_tensor(self.state[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.action[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.next_state[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.reward[ind], dtype=tf.float32),\n",
        "            tf.convert_to_tensor(self.not_done[ind], dtype=tf.float32)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6xvdeuChkTE"
      },
      "source": [
        "# Training and evaluating procedures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhStR6soa5dl"
      },
      "source": [
        "We define a function for evaluating the policy. When evaluating the trained policy, we evaluate the performance without the exploration noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qLp8X2BDbBJh"
      },
      "outputs": [],
      "source": [
        "def evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim):\n",
        "\n",
        "    state_test = env_test.reset()\n",
        "    return_epi_test = 0\n",
        "    for t_test in range(int(args['max_episode_len'])):\n",
        "        if args['render_env']:\n",
        "                env_test.render()\n",
        "\n",
        "        action_test = agent.select_action(np.reshape(state_test, (1, state_dim)))\n",
        "        state_test2, reward_test, terminal_test, info_test = env_test.step(action_test)\n",
        "        state_test = state_test2\n",
        "        return_epi_test = return_epi_test + reward_test\n",
        "        if terminal_test:\n",
        "            break\n",
        "\n",
        "    print('test_iter:{:d}, nn:{:d}, return_epi_test: {:d}'.format(int(test_iter), int(test_n),\n",
        "                                                                      int(return_epi_test)))\n",
        "\n",
        "    return return_epi_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJgOjkxObQbL"
      },
      "source": [
        "Below is the training procedure. We collect samples through trials and errors and store them in the replay buffer. The policy is trained once after every one time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q-LnStctbHOC"
      },
      "outputs": [],
      "source": [
        "def train(env, env_test, agent, args, index):\n",
        "    # Initialize replay memory\n",
        "    total_step_cnt = 0\n",
        "    epi_cnt = 0\n",
        "    test_iter = 0\n",
        "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "    while total_step_cnt < int(args['total_step_num']):\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        T_end = False\n",
        "\n",
        "        for t in range(int(args['max_episode_len'])):\n",
        "            # Select action randomly or according to policy\n",
        "            if total_step_cnt < int(args['start_timesteps']):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.select_action(np.array(state))\n",
        "\n",
        "            state2, reward, terminal, info = env.step(action)\n",
        "            terminal_bool = float(terminal) if t < int(args['max_episode_len']) else 0\n",
        "\n",
        "            # Store data in replay buffer\n",
        "            replay_buffer.add(state, action, state2, reward, terminal_bool)\n",
        "\n",
        "            # Train agent after collecting sufficient data\n",
        "            if total_step_cnt >= int(args['start_timesteps']):\n",
        "                for i in range(int(args['update_freq'])):\n",
        "                    agent.train(replay_buffer, int(args['batch_size']))\n",
        "\n",
        "            if t == int(args['max_episode_len']) - 1:\n",
        "                T_end = True\n",
        "\n",
        "            state = state2\n",
        "            ep_reward += reward\n",
        "            total_step_cnt += 1\n",
        "\n",
        "            # Evaluate the deterministic policy\n",
        "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
        "                print('total_step_cnt', total_step_cnt)\n",
        "                print('evaluating the deterministic policy...')\n",
        "                for test_n in range(int(args['test_num'])):\n",
        "                    return_epi_test = evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim)\n",
        "\n",
        "                    # Store the average of returns over the test episodes\n",
        "                    return_test[test_iter] = return_test[test_iter] + return_epi_test / float(args['test_num'])\n",
        "\n",
        "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
        "                test_iter += 1\n",
        "\n",
        "            if total_step_cnt % int(args['model_save_freq']) == 0:\n",
        "                agent.save_model(iter=test_iter, seed=int(index), env_name=args['env'])\n",
        "\n",
        "            if terminal or T_end:\n",
        "                epi_cnt += 1\n",
        "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(int(ep_reward), epi_cnt, total_step_cnt))\n",
        "                break\n",
        "\n",
        "    return return_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxW9O6IUfPeZ"
      },
      "source": [
        "Main funciton. To manage the hyperparamters of TD3, we use argeparse.\n",
        "We use a task in OpenAI Gym (https://gym.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJU8BAebYYO",
        "outputId": "499f11ee-dd14-4639-f545-5e5725172dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial Number: 0\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1648\n",
            "test_iter:0, nn:1, return_epi_test: -1372\n",
            "test_iter:0, nn:2, return_epi_test: -1541\n",
            "test_iter:0, nn:3, return_epi_test: -1255\n",
            "test_iter:0, nn:4, return_epi_test: -1666\n",
            "test_iter:0, nn:5, return_epi_test: -1676\n",
            "test_iter:0, nn:6, return_epi_test: -1258\n",
            "test_iter:0, nn:7, return_epi_test: -1588\n",
            "test_iter:0, nn:8, return_epi_test: -1231\n",
            "test_iter:0, nn:9, return_epi_test: -1688\n",
            "return_test[0] -1492\n",
            "| Reward: -1513 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -977 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1331 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -772 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -1506 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1744 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -874 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1589 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -759 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1762 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -1334 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -876 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1450 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1228 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -878 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -774 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -887 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -866 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1638 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -948 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -1061 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1665 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -1641 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1524 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1077\n",
            "test_iter:1, nn:1, return_epi_test: -1290\n",
            "test_iter:1, nn:2, return_epi_test: -1306\n",
            "test_iter:1, nn:3, return_epi_test: -1153\n",
            "test_iter:1, nn:4, return_epi_test: -1226\n",
            "test_iter:1, nn:5, return_epi_test: -1131\n",
            "test_iter:1, nn:6, return_epi_test: -1367\n",
            "test_iter:1, nn:7, return_epi_test: -1303\n",
            "test_iter:1, nn:8, return_epi_test: -1472\n",
            "test_iter:1, nn:9, return_epi_test: -1257\n",
            "return_test[1] -1258\n",
            "| Reward: -883 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1699 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1692 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1071 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -964 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1791 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -1415 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1438 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -762 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -1496 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -1639 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1329 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1686 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -977 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1126 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -1316 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -1697 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -1725 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -872 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -974 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -1070 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -918 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -1443 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -980 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -1040 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1174\n",
            "test_iter:2, nn:1, return_epi_test: -1795\n",
            "test_iter:2, nn:2, return_epi_test: -1684\n",
            "test_iter:2, nn:3, return_epi_test: -1485\n",
            "test_iter:2, nn:4, return_epi_test: -1373\n",
            "test_iter:2, nn:5, return_epi_test: -1595\n",
            "test_iter:2, nn:6, return_epi_test: -1417\n",
            "test_iter:2, nn:7, return_epi_test: -1129\n",
            "test_iter:2, nn:8, return_epi_test: -1361\n",
            "test_iter:2, nn:9, return_epi_test: -1771\n",
            "return_test[2] -1478\n",
            "models is saved for iteration 3\n",
            "| Reward: -881 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1646 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1395 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1753 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1756 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1590 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1543 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1414 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1492 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1480 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1478 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1403 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1388 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1330 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1302 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1246 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1214 | Episode: 66 | Total step num: 13200 |\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7224\\3518859770.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n\u001b[0;32m     63\u001b[0m                     default='./results/trials/td3/trials_td3_')\n\u001b[0;32m     64\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--trial-idx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index of trials'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--monitor-dir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'directory for recording'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'results/video/td3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--model-save-freq'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'frequency of evaluating the policy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--model-folder'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model/td3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--start_timesteps\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# How many time steps purely random policy is run for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7224\\935908211.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(env, env_test, agent, args, index)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Train agent after collecting sufficient data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal_step_cnt\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_timesteps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'update_freq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_episode_len'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mT_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7224\\3356417789.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[0;32m     75\u001b[0m                           \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_Q2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mgrads_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_critic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_it\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape_actor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m             \u001b[1;34m\"experimental_aggregate_gradients\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m         )\n\u001b[0;32m   1228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    654\u001b[0m             \u001b[1;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                     \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mesh\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_with_dtensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[1;31m# Skip any usage of strategy logic for DTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1260\u001b[1;33m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[0;32m   1261\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mThe\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n\u001b[0;32m     54\u001b[0m         fn, args=args, kwargs=kwargs)\n",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m             distribution.extended.update(\n\u001b[0m\u001b[0;32m   1353\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m             )\n\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2988\u001b[0m         _get_default_replica_context()):\n\u001b[0;32m   2989\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   2990\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   2991\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2992\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2993\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m       return self._replica_ctx_update(\n\u001b[0;32m   2995\u001b[0m           var, fn, args=args, kwargs=kwargs, group=group)\n",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4059\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4060\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4061\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4062\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4068\u001b[0m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4069\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4071\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4072\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[1;34m\"`optimizer.build(variables)` with the full list of trainable \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[1;34m\"variables before the training loop or use legacy optimizer \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[1;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mv_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_velocity_hats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mv_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1480\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1843\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1844\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m   \"\"\"\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\12694\\miniconda3\\envs\\rl_homework_gym\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7327\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7329\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7330\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7331\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7332\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7333\u001b[0m       return mul_eager_fallback(\n\u001b[0;32m   7334\u001b[0m           x, y, name=name, ctx=_ctx)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# run parameters\n",
        "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v1}')\n",
        "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {Pendulum-v1}')\n",
        "parser.add_argument('--random-seed', help='random seed for repeatability',type = int, default=1)\n",
        "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=1001)\n",
        "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
        "parser.add_argument('--trial-num', help='number of trials', default=3)\n",
        "# parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
        "parser.add_argument('--total-step-num', help='total number of time steps', default=25000)\n",
        "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=5000)\n",
        "parser.add_argument('--test-num', help='number of test episodes', default=10)\n",
        "\n",
        "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
        "                    default='./results/trials/td3/trials_td3_')\n",
        "parser.add_argument('--trial-idx', help='index of trials', default=0)\n",
        "parser.add_argument('--monitor-dir', help='directory for recording', default='results/video/td3')\n",
        "parser.add_argument('--model-save-freq', help='frequency of evaluating the policy', default=10000)\n",
        "parser.add_argument('--model-folder',  default='./model/td3')\n",
        "\n",
        "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)  # How many time steps purely random policy is run for\n",
        "parser.add_argument(\"--expl_noise\", default=0.1, type=float)  # Std of Gaussian exploration noise\n",
        "parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
        "parser.add_argument(\"--update_freq\", default=1, type=int)  # Number of policy updates\n",
        "\n",
        "parser.set_defaults(render_env=False)\n",
        "\n",
        "args_tmp, unknown = parser.parse_known_args()\n",
        "\n",
        "if args_tmp.env is None:\n",
        "    env_dict = {0 : \"Pendulum-v1\",\n",
        "    }\n",
        "    args_tmp.env = env_dict[args_tmp.env_id]\n",
        "args = vars(args_tmp)\n",
        "\n",
        "return_set=[]\n",
        "for ite in range(int(args['trial_num'])):\n",
        "    print('Trial Number:', ite)\n",
        "\n",
        "    index = int(ite) + int(args['trial_idx'])\n",
        "    env = gym.make(args['env'])\n",
        "\n",
        "    np.random.seed(index)\n",
        "    env.seed(index)\n",
        "\n",
        "    env_test = gym.make(args['env'])\n",
        "    env_test.seed(index)\n",
        "\n",
        "    print('action_space.shape', env.action_space.shape)\n",
        "    print('observation_space.shape', env.observation_space.shape)\n",
        "    action_bound = float(env.action_space.high[0])\n",
        "\n",
        "    assert (env.action_space.high[0] == -env.action_space.low[0])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    agent = TD3(state_dim=state_dim, action_dim=action_dim, max_action=action_bound)\n",
        "\n",
        "    step_R_i = train(env, env_test, agent, args, index)\n",
        "    return_set.append(step_R_i)\n",
        "\n",
        "    result_path = \"./results/trials/td3\"\n",
        "    result_filename = args['result_file'] +  \\\n",
        "                      '_update_freq_' + str(int(args['update_freq'])) + '_' + args['env'] +  \\\n",
        "                      '_trial_idx_' + str(index) + '.txt'\n",
        "    try:\n",
        "        import pathlib\n",
        "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
        "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
        "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
        "    except:\n",
        "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdSCbpbYhfME"
      },
      "source": [
        "We plot the results of the training using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WjNBeCJPhdQY"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "figure(figsize=(7, 6))\n",
        "\n",
        "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
        "\n",
        "mean = np.mean(np.asarray(return_set), axis=0)\n",
        "std = np.std(np.asarray(return_set), axis=0)\n",
        "color = 'b'\n",
        "label = 'TD3'\n",
        "plt.plot(t, mean, color, label=label)\n",
        "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
        "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
        "\n",
        "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
        "plt.ylabel('Return', fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=14)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# run sac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(env, env_test, agent, args, index):\n",
        "    # Initialize replay memory\n",
        "    total_step_cnt = 0\n",
        "    epi_cnt = 0\n",
        "    test_iter = 0\n",
        "    return_test = np.zeros((np.ceil(int(args['total_step_num']) / int(args['eval_step_freq'])).astype('int') + 1))\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "\n",
        "    while total_step_cnt < int(args['total_step_num']):\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        T_end = False\n",
        "\n",
        "        for t in range(int(args['max_episode_len'])):\n",
        "            # Select action randomly or according to policy\n",
        "            if total_step_cnt < int(args['start_timesteps']):\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = agent.select_action(np.array(state))\n",
        "\n",
        "            state2, reward, terminal, info = env.step(action)\n",
        "            terminal_bool = float(terminal) if t < int(args['max_episode_len']) else 0\n",
        "\n",
        "            # Store data in replay buffer\n",
        "            replay_buffer.add(state, action, state2, reward, terminal_bool)\n",
        "\n",
        "            # Train agent after collecting sufficient data\n",
        "            if total_step_cnt >= int(args['start_timesteps']):\n",
        "                for i in range(int(args['update_freq'])):\n",
        "                    agent.train(replay_buffer, int(args['batch_size']))\n",
        "\n",
        "            if t == int(args['max_episode_len']) - 1:\n",
        "                T_end = True\n",
        "\n",
        "            state = state2\n",
        "            ep_reward += reward\n",
        "            total_step_cnt += 1\n",
        "\n",
        "            # Evaluate the deterministic policy\n",
        "            if total_step_cnt >= test_iter * int(args['eval_step_freq']) or total_step_cnt == 1:\n",
        "                print('total_step_cnt', total_step_cnt)\n",
        "                print('evaluating the deterministic policy...')\n",
        "                for test_n in range(int(args['test_num'])):\n",
        "                    return_epi_test = evaluate_greedy(env_test, agent, args, test_iter, test_n, state_dim)\n",
        "\n",
        "                    # Store the average of returns over the test episodes\n",
        "                    return_test[test_iter] = return_test[test_iter] + return_epi_test / float(args['test_num'])\n",
        "\n",
        "                print('return_test[{:d}] {:d}'.format(int(test_iter), int(return_test[test_iter])))\n",
        "                test_iter += 1\n",
        "\n",
        "            if total_step_cnt % int(args['model_save_freq']) == 0:\n",
        "                agent.save_model(iter=test_iter, seed=int(index), env_name=args['env'])\n",
        "\n",
        "            if terminal or T_end:\n",
        "                epi_cnt += 1\n",
        "                print('| Reward: {:d} | Episode: {:d} | Total step num: {:d} |'.format(int(ep_reward), epi_cnt, total_step_cnt))\n",
        "                break\n",
        "\n",
        "    return return_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial Number: 0\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1817\n",
            "test_iter:0, nn:1, return_epi_test: -936\n",
            "test_iter:0, nn:2, return_epi_test: -1729\n",
            "test_iter:0, nn:3, return_epi_test: -1027\n",
            "test_iter:0, nn:4, return_epi_test: -1518\n",
            "test_iter:0, nn:5, return_epi_test: -1598\n",
            "test_iter:0, nn:6, return_epi_test: -987\n",
            "test_iter:0, nn:7, return_epi_test: -1754\n",
            "test_iter:0, nn:8, return_epi_test: -741\n",
            "test_iter:0, nn:9, return_epi_test: -1821\n",
            "return_test[0] -1393\n",
            "| Reward: -1724 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -1127 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1560 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -880 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -1651 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1502 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -768 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1636 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -892 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1800 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -1488 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -861 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1346 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1244 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -876 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -971 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -998 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -1083 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1622 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -950 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -848 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1649 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -1631 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1758 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1261\n",
            "test_iter:1, nn:1, return_epi_test: -1034\n",
            "test_iter:1, nn:2, return_epi_test: -1187\n",
            "test_iter:1, nn:3, return_epi_test: -1190\n",
            "test_iter:1, nn:4, return_epi_test: -900\n",
            "test_iter:1, nn:5, return_epi_test: -1082\n",
            "test_iter:1, nn:6, return_epi_test: -964\n",
            "test_iter:1, nn:7, return_epi_test: -1032\n",
            "test_iter:1, nn:8, return_epi_test: -1595\n",
            "test_iter:1, nn:9, return_epi_test: -1030\n",
            "return_test[1] -1127\n",
            "| Reward: -903 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1488 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1349 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1196 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -1071 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1688 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -1428 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1372 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -809 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -1213 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -1588 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1617 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1735 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -1069 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1064 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -1292 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -1560 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -1647 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -881 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -850 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -1050 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1238 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -1291 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -868 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -890 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -962\n",
            "test_iter:2, nn:1, return_epi_test: -1823\n",
            "test_iter:2, nn:2, return_epi_test: -1623\n",
            "test_iter:2, nn:3, return_epi_test: -1380\n",
            "test_iter:2, nn:4, return_epi_test: -863\n",
            "test_iter:2, nn:5, return_epi_test: -1643\n",
            "test_iter:2, nn:6, return_epi_test: -1635\n",
            "test_iter:2, nn:7, return_epi_test: -1065\n",
            "test_iter:2, nn:8, return_epi_test: -1006\n",
            "test_iter:2, nn:9, return_epi_test: -1798\n",
            "return_test[2] -1380\n",
            "models is saved for iteration 3\n",
            "| Reward: -877 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1625 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1134 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1404 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1708 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1763 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1810 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1646 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1619 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1495 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1508 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1430 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1485 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1504 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1435 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1199 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1155 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -661 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -916 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -1497 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -410 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -270 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -139 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -1392 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -4 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -296\n",
            "test_iter:3, nn:1, return_epi_test: -305\n",
            "test_iter:3, nn:2, return_epi_test: -6\n",
            "test_iter:3, nn:3, return_epi_test: -1055\n",
            "test_iter:3, nn:4, return_epi_test: -287\n",
            "test_iter:3, nn:5, return_epi_test: -1141\n",
            "test_iter:3, nn:6, return_epi_test: -972\n",
            "test_iter:3, nn:7, return_epi_test: -1061\n",
            "test_iter:3, nn:8, return_epi_test: -139\n",
            "test_iter:3, nn:9, return_epi_test: -133\n",
            "return_test[3] -539\n",
            "| Reward: -263 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -8 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -264 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -138 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -269 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -135 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -346 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -1163 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -1068 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: -1001 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -133 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -668 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -372 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -128 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -132 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -124 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -132 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -255 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -117 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -128 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -3 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -249 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -133 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -120 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -253 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -300\n",
            "test_iter:4, nn:1, return_epi_test: -356\n",
            "test_iter:4, nn:2, return_epi_test: -129\n",
            "test_iter:4, nn:3, return_epi_test: -2\n",
            "test_iter:4, nn:4, return_epi_test: -129\n",
            "test_iter:4, nn:5, return_epi_test: -122\n",
            "test_iter:4, nn:6, return_epi_test: -117\n",
            "test_iter:4, nn:7, return_epi_test: -127\n",
            "test_iter:4, nn:8, return_epi_test: -4\n",
            "test_iter:4, nn:9, return_epi_test: -2\n",
            "return_test[4] -129\n",
            "models is saved for iteration 5\n",
            "| Reward: -228 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -120 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -117 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -255 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -238 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -353 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -124 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -121 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -236 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -127 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -365 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -130 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -132 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -239 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -119 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -120 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -338 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -248 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -1070 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -118 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -128 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -247 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -119 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -119 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -245 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -342\n",
            "test_iter:5, nn:1, return_epi_test: -994\n",
            "test_iter:5, nn:2, return_epi_test: -118\n",
            "test_iter:5, nn:3, return_epi_test: -122\n",
            "test_iter:5, nn:4, return_epi_test: -124\n",
            "test_iter:5, nn:5, return_epi_test: -229\n",
            "test_iter:5, nn:6, return_epi_test: -925\n",
            "test_iter:5, nn:7, return_epi_test: -123\n",
            "test_iter:5, nn:8, return_epi_test: -120\n",
            "test_iter:5, nn:9, return_epi_test: -118\n",
            "return_test[5] -322\n",
            "| Reward: -1016 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.0 was saved.\n",
            "Trial Number: 1\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1269\n",
            "test_iter:0, nn:1, return_epi_test: -1462\n",
            "test_iter:0, nn:2, return_epi_test: -1374\n",
            "test_iter:0, nn:3, return_epi_test: -1624\n",
            "test_iter:0, nn:4, return_epi_test: -1174\n",
            "test_iter:0, nn:5, return_epi_test: -1472\n",
            "test_iter:0, nn:6, return_epi_test: -1333\n",
            "test_iter:0, nn:7, return_epi_test: -1414\n",
            "test_iter:0, nn:8, return_epi_test: -1220\n",
            "test_iter:0, nn:9, return_epi_test: -1564\n",
            "return_test[0] -1391\n",
            "| Reward: -1383 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -1307 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -869 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -1572 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -1160 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1572 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -1250 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1637 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -889 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1494 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -859 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -1333 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1516 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1237 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -1336 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -1597 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -1365 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -877 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1091 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -1359 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -1370 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1224 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -744 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1550 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1074\n",
            "test_iter:1, nn:1, return_epi_test: -1412\n",
            "test_iter:1, nn:2, return_epi_test: -1681\n",
            "test_iter:1, nn:3, return_epi_test: -1459\n",
            "test_iter:1, nn:4, return_epi_test: -1388\n",
            "test_iter:1, nn:5, return_epi_test: -1609\n",
            "test_iter:1, nn:6, return_epi_test: -1447\n",
            "test_iter:1, nn:7, return_epi_test: -1034\n",
            "test_iter:1, nn:8, return_epi_test: -966\n",
            "test_iter:1, nn:9, return_epi_test: -1437\n",
            "return_test[1] -1351\n",
            "| Reward: -1182 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1335 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1441 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1290 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -970 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1070 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -897 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -1316 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -911 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -964 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -901 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1303 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1003 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -1121 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1424 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -968 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -964 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -1382 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -1061 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -1216 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -967 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1353 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -876 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -1727 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -883 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1620\n",
            "test_iter:2, nn:1, return_epi_test: -1383\n",
            "test_iter:2, nn:2, return_epi_test: -907\n",
            "test_iter:2, nn:3, return_epi_test: -1451\n",
            "test_iter:2, nn:4, return_epi_test: -1183\n",
            "test_iter:2, nn:5, return_epi_test: -1241\n",
            "test_iter:2, nn:6, return_epi_test: -1333\n",
            "test_iter:2, nn:7, return_epi_test: -1279\n",
            "test_iter:2, nn:8, return_epi_test: -1151\n",
            "test_iter:2, nn:9, return_epi_test: -1074\n",
            "return_test[2] -1262\n",
            "models is saved for iteration 3\n",
            "| Reward: -1469 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -1027 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1169 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1464 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1782 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1830 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1773 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1833 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1707 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1554 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1479 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1621 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1549 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1506 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1447 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -1480 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -1171 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -139 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -1493 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -1179 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -1493 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -138 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -1161 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -451 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -138 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -138\n",
            "test_iter:3, nn:1, return_epi_test: -129\n",
            "test_iter:3, nn:2, return_epi_test: -4\n",
            "test_iter:3, nn:3, return_epi_test: -134\n",
            "test_iter:3, nn:4, return_epi_test: -132\n",
            "test_iter:3, nn:5, return_epi_test: -276\n",
            "test_iter:3, nn:6, return_epi_test: -135\n",
            "test_iter:3, nn:7, return_epi_test: -1402\n",
            "test_iter:3, nn:8, return_epi_test: -136\n",
            "test_iter:3, nn:9, return_epi_test: -138\n",
            "return_test[3] -262\n",
            "| Reward: -268 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -290 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -1491 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -131 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -386 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -266 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -380 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -3 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -139 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: -362 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -1213 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -116 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -133 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -119 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -133 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -937 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -244 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -136 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -126 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -127 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -3 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -116 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -121 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -132 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -130 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -131\n",
            "test_iter:4, nn:1, return_epi_test: -239\n",
            "test_iter:4, nn:2, return_epi_test: -121\n",
            "test_iter:4, nn:3, return_epi_test: -118\n",
            "test_iter:4, nn:4, return_epi_test: -131\n",
            "test_iter:4, nn:5, return_epi_test: -252\n",
            "test_iter:4, nn:6, return_epi_test: -2\n",
            "test_iter:4, nn:7, return_epi_test: -231\n",
            "test_iter:4, nn:8, return_epi_test: -2\n",
            "test_iter:4, nn:9, return_epi_test: -243\n",
            "return_test[4] -147\n",
            "models is saved for iteration 5\n",
            "| Reward: -360 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -242 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -118 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -247 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -2 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -116 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -2 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -4 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -126 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -233 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -127 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -129 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -117 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -339 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -119 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -2 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -347 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -226 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -225 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -2 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -240 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -116 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -118 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -231 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -2 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -130\n",
            "test_iter:5, nn:1, return_epi_test: -2\n",
            "test_iter:5, nn:2, return_epi_test: -118\n",
            "test_iter:5, nn:3, return_epi_test: -117\n",
            "test_iter:5, nn:4, return_epi_test: -241\n",
            "test_iter:5, nn:5, return_epi_test: -121\n",
            "test_iter:5, nn:6, return_epi_test: -237\n",
            "test_iter:5, nn:7, return_epi_test: -126\n",
            "test_iter:5, nn:8, return_epi_test: -3\n",
            "test_iter:5, nn:9, return_epi_test: -129\n",
            "return_test[5] -122\n",
            "| Reward: -124 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.1 was saved.\n",
            "Trial Number: 2\n",
            "action_space.shape (1,)\n",
            "observation_space.shape (3,)\n",
            "total_step_cnt 1\n",
            "evaluating the deterministic policy...\n",
            "test_iter:0, nn:0, return_epi_test: -1087\n",
            "test_iter:0, nn:1, return_epi_test: -922\n",
            "test_iter:0, nn:2, return_epi_test: -1581\n",
            "test_iter:0, nn:3, return_epi_test: -1158\n",
            "test_iter:0, nn:4, return_epi_test: -1195\n",
            "test_iter:0, nn:5, return_epi_test: -1238\n",
            "test_iter:0, nn:6, return_epi_test: -1121\n",
            "test_iter:0, nn:7, return_epi_test: -1211\n",
            "test_iter:0, nn:8, return_epi_test: -1175\n",
            "test_iter:0, nn:9, return_epi_test: -1209\n",
            "return_test[0] -1190\n",
            "| Reward: -1306 | Episode: 1 | Total step num: 200 |\n",
            "| Reward: -1068 | Episode: 2 | Total step num: 400 |\n",
            "| Reward: -1506 | Episode: 3 | Total step num: 600 |\n",
            "| Reward: -1171 | Episode: 4 | Total step num: 800 |\n",
            "| Reward: -924 | Episode: 5 | Total step num: 1000 |\n",
            "| Reward: -1473 | Episode: 6 | Total step num: 1200 |\n",
            "| Reward: -1109 | Episode: 7 | Total step num: 1400 |\n",
            "| Reward: -1307 | Episode: 8 | Total step num: 1600 |\n",
            "| Reward: -1200 | Episode: 9 | Total step num: 1800 |\n",
            "| Reward: -1005 | Episode: 10 | Total step num: 2000 |\n",
            "| Reward: -1159 | Episode: 11 | Total step num: 2200 |\n",
            "| Reward: -1568 | Episode: 12 | Total step num: 2400 |\n",
            "| Reward: -1085 | Episode: 13 | Total step num: 2600 |\n",
            "| Reward: -1818 | Episode: 14 | Total step num: 2800 |\n",
            "| Reward: -1541 | Episode: 15 | Total step num: 3000 |\n",
            "| Reward: -1305 | Episode: 16 | Total step num: 3200 |\n",
            "| Reward: -1179 | Episode: 17 | Total step num: 3400 |\n",
            "| Reward: -1113 | Episode: 18 | Total step num: 3600 |\n",
            "| Reward: -1708 | Episode: 19 | Total step num: 3800 |\n",
            "| Reward: -883 | Episode: 20 | Total step num: 4000 |\n",
            "| Reward: -1294 | Episode: 21 | Total step num: 4200 |\n",
            "| Reward: -1784 | Episode: 22 | Total step num: 4400 |\n",
            "| Reward: -1561 | Episode: 23 | Total step num: 4600 |\n",
            "| Reward: -1819 | Episode: 24 | Total step num: 4800 |\n",
            "total_step_cnt 5000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:1, nn:0, return_epi_test: -1097\n",
            "test_iter:1, nn:1, return_epi_test: -1253\n",
            "test_iter:1, nn:2, return_epi_test: -1212\n",
            "test_iter:1, nn:3, return_epi_test: -1634\n",
            "test_iter:1, nn:4, return_epi_test: -1783\n",
            "test_iter:1, nn:5, return_epi_test: -1033\n",
            "test_iter:1, nn:6, return_epi_test: -1116\n",
            "test_iter:1, nn:7, return_epi_test: -1169\n",
            "test_iter:1, nn:8, return_epi_test: -1224\n",
            "test_iter:1, nn:9, return_epi_test: -1210\n",
            "return_test[1] -1273\n",
            "| Reward: -976 | Episode: 25 | Total step num: 5000 |\n",
            "| Reward: -1088 | Episode: 26 | Total step num: 5200 |\n",
            "| Reward: -1503 | Episode: 27 | Total step num: 5400 |\n",
            "| Reward: -1083 | Episode: 28 | Total step num: 5600 |\n",
            "| Reward: -1517 | Episode: 29 | Total step num: 5800 |\n",
            "| Reward: -1299 | Episode: 30 | Total step num: 6000 |\n",
            "| Reward: -1007 | Episode: 31 | Total step num: 6200 |\n",
            "| Reward: -942 | Episode: 32 | Total step num: 6400 |\n",
            "| Reward: -1579 | Episode: 33 | Total step num: 6600 |\n",
            "| Reward: -1167 | Episode: 34 | Total step num: 6800 |\n",
            "| Reward: -742 | Episode: 35 | Total step num: 7000 |\n",
            "| Reward: -1169 | Episode: 36 | Total step num: 7200 |\n",
            "| Reward: -1295 | Episode: 37 | Total step num: 7400 |\n",
            "| Reward: -895 | Episode: 38 | Total step num: 7600 |\n",
            "| Reward: -1071 | Episode: 39 | Total step num: 7800 |\n",
            "| Reward: -1660 | Episode: 40 | Total step num: 8000 |\n",
            "| Reward: -974 | Episode: 41 | Total step num: 8200 |\n",
            "| Reward: -971 | Episode: 42 | Total step num: 8400 |\n",
            "| Reward: -1323 | Episode: 43 | Total step num: 8600 |\n",
            "| Reward: -1307 | Episode: 44 | Total step num: 8800 |\n",
            "| Reward: -1554 | Episode: 45 | Total step num: 9000 |\n",
            "| Reward: -1157 | Episode: 46 | Total step num: 9200 |\n",
            "| Reward: -1083 | Episode: 47 | Total step num: 9400 |\n",
            "| Reward: -1271 | Episode: 48 | Total step num: 9600 |\n",
            "| Reward: -1441 | Episode: 49 | Total step num: 9800 |\n",
            "total_step_cnt 10000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:2, nn:0, return_epi_test: -1107\n",
            "test_iter:2, nn:1, return_epi_test: -1633\n",
            "test_iter:2, nn:2, return_epi_test: -1089\n",
            "test_iter:2, nn:3, return_epi_test: -1481\n",
            "test_iter:2, nn:4, return_epi_test: -1115\n",
            "test_iter:2, nn:5, return_epi_test: -1181\n",
            "test_iter:2, nn:6, return_epi_test: -1387\n",
            "test_iter:2, nn:7, return_epi_test: -1059\n",
            "test_iter:2, nn:8, return_epi_test: -1380\n",
            "test_iter:2, nn:9, return_epi_test: -1189\n",
            "return_test[2] -1262\n",
            "models is saved for iteration 3\n",
            "| Reward: -886 | Episode: 50 | Total step num: 10000 |\n",
            "| Reward: -862 | Episode: 51 | Total step num: 10200 |\n",
            "| Reward: -1522 | Episode: 52 | Total step num: 10400 |\n",
            "| Reward: -1331 | Episode: 53 | Total step num: 10600 |\n",
            "| Reward: -1800 | Episode: 54 | Total step num: 10800 |\n",
            "| Reward: -1789 | Episode: 55 | Total step num: 11000 |\n",
            "| Reward: -1733 | Episode: 56 | Total step num: 11200 |\n",
            "| Reward: -1693 | Episode: 57 | Total step num: 11400 |\n",
            "| Reward: -1787 | Episode: 58 | Total step num: 11600 |\n",
            "| Reward: -1782 | Episode: 59 | Total step num: 11800 |\n",
            "| Reward: -1468 | Episode: 60 | Total step num: 12000 |\n",
            "| Reward: -1503 | Episode: 61 | Total step num: 12200 |\n",
            "| Reward: -1450 | Episode: 62 | Total step num: 12400 |\n",
            "| Reward: -1433 | Episode: 63 | Total step num: 12600 |\n",
            "| Reward: -1281 | Episode: 64 | Total step num: 12800 |\n",
            "| Reward: -991 | Episode: 65 | Total step num: 13000 |\n",
            "| Reward: -735 | Episode: 66 | Total step num: 13200 |\n",
            "| Reward: -814 | Episode: 67 | Total step num: 13400 |\n",
            "| Reward: -791 | Episode: 68 | Total step num: 13600 |\n",
            "| Reward: -550 | Episode: 69 | Total step num: 13800 |\n",
            "| Reward: -406 | Episode: 70 | Total step num: 14000 |\n",
            "| Reward: -403 | Episode: 71 | Total step num: 14200 |\n",
            "| Reward: -139 | Episode: 72 | Total step num: 14400 |\n",
            "| Reward: -1099 | Episode: 73 | Total step num: 14600 |\n",
            "| Reward: -1103 | Episode: 74 | Total step num: 14800 |\n",
            "total_step_cnt 15000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:3, nn:0, return_epi_test: -133\n",
            "test_iter:3, nn:1, return_epi_test: -985\n",
            "test_iter:3, nn:2, return_epi_test: -390\n",
            "test_iter:3, nn:3, return_epi_test: -133\n",
            "test_iter:3, nn:4, return_epi_test: -133\n",
            "test_iter:3, nn:5, return_epi_test: -265\n",
            "test_iter:3, nn:6, return_epi_test: -644\n",
            "test_iter:3, nn:7, return_epi_test: -132\n",
            "test_iter:3, nn:8, return_epi_test: -133\n",
            "test_iter:3, nn:9, return_epi_test: -428\n",
            "return_test[3] -338\n",
            "| Reward: -277 | Episode: 75 | Total step num: 15000 |\n",
            "| Reward: -136 | Episode: 76 | Total step num: 15200 |\n",
            "| Reward: -136 | Episode: 77 | Total step num: 15400 |\n",
            "| Reward: -133 | Episode: 78 | Total step num: 15600 |\n",
            "| Reward: -381 | Episode: 79 | Total step num: 15800 |\n",
            "| Reward: -3 | Episode: 80 | Total step num: 16000 |\n",
            "| Reward: -126 | Episode: 81 | Total step num: 16200 |\n",
            "| Reward: -239 | Episode: 82 | Total step num: 16400 |\n",
            "| Reward: -138 | Episode: 83 | Total step num: 16600 |\n",
            "| Reward: -2 | Episode: 84 | Total step num: 16800 |\n",
            "| Reward: -353 | Episode: 85 | Total step num: 17000 |\n",
            "| Reward: -3 | Episode: 86 | Total step num: 17200 |\n",
            "| Reward: -1323 | Episode: 87 | Total step num: 17400 |\n",
            "| Reward: -1512 | Episode: 88 | Total step num: 17600 |\n",
            "| Reward: -1058 | Episode: 89 | Total step num: 17800 |\n",
            "| Reward: -133 | Episode: 90 | Total step num: 18000 |\n",
            "| Reward: -518 | Episode: 91 | Total step num: 18200 |\n",
            "| Reward: -263 | Episode: 92 | Total step num: 18400 |\n",
            "| Reward: -125 | Episode: 93 | Total step num: 18600 |\n",
            "| Reward: -117 | Episode: 94 | Total step num: 18800 |\n",
            "| Reward: -243 | Episode: 95 | Total step num: 19000 |\n",
            "| Reward: -240 | Episode: 96 | Total step num: 19200 |\n",
            "| Reward: -133 | Episode: 97 | Total step num: 19400 |\n",
            "| Reward: -115 | Episode: 98 | Total step num: 19600 |\n",
            "| Reward: -120 | Episode: 99 | Total step num: 19800 |\n",
            "total_step_cnt 20000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:4, nn:0, return_epi_test: -7\n",
            "test_iter:4, nn:1, return_epi_test: -3\n",
            "test_iter:4, nn:2, return_epi_test: -117\n",
            "test_iter:4, nn:3, return_epi_test: -130\n",
            "test_iter:4, nn:4, return_epi_test: -326\n",
            "test_iter:4, nn:5, return_epi_test: -128\n",
            "test_iter:4, nn:6, return_epi_test: -126\n",
            "test_iter:4, nn:7, return_epi_test: -124\n",
            "test_iter:4, nn:8, return_epi_test: -504\n",
            "test_iter:4, nn:9, return_epi_test: -3\n",
            "return_test[4] -147\n",
            "models is saved for iteration 5\n",
            "| Reward: -129 | Episode: 100 | Total step num: 20000 |\n",
            "| Reward: -2 | Episode: 101 | Total step num: 20200 |\n",
            "| Reward: -306 | Episode: 102 | Total step num: 20400 |\n",
            "| Reward: -238 | Episode: 103 | Total step num: 20600 |\n",
            "| Reward: -229 | Episode: 104 | Total step num: 20800 |\n",
            "| Reward: -115 | Episode: 105 | Total step num: 21000 |\n",
            "| Reward: -128 | Episode: 106 | Total step num: 21200 |\n",
            "| Reward: -241 | Episode: 107 | Total step num: 21400 |\n",
            "| Reward: -115 | Episode: 108 | Total step num: 21600 |\n",
            "| Reward: -128 | Episode: 109 | Total step num: 21800 |\n",
            "| Reward: -234 | Episode: 110 | Total step num: 22000 |\n",
            "| Reward: -347 | Episode: 111 | Total step num: 22200 |\n",
            "| Reward: -124 | Episode: 112 | Total step num: 22400 |\n",
            "| Reward: -3 | Episode: 113 | Total step num: 22600 |\n",
            "| Reward: -117 | Episode: 114 | Total step num: 22800 |\n",
            "| Reward: -132 | Episode: 115 | Total step num: 23000 |\n",
            "| Reward: -356 | Episode: 116 | Total step num: 23200 |\n",
            "| Reward: -122 | Episode: 117 | Total step num: 23400 |\n",
            "| Reward: -128 | Episode: 118 | Total step num: 23600 |\n",
            "| Reward: -244 | Episode: 119 | Total step num: 23800 |\n",
            "| Reward: -231 | Episode: 120 | Total step num: 24000 |\n",
            "| Reward: -119 | Episode: 121 | Total step num: 24200 |\n",
            "| Reward: -235 | Episode: 122 | Total step num: 24400 |\n",
            "| Reward: -126 | Episode: 123 | Total step num: 24600 |\n",
            "| Reward: -127 | Episode: 124 | Total step num: 24800 |\n",
            "total_step_cnt 25000\n",
            "evaluating the deterministic policy...\n",
            "test_iter:5, nn:0, return_epi_test: -131\n",
            "test_iter:5, nn:1, return_epi_test: -241\n",
            "test_iter:5, nn:2, return_epi_test: -128\n",
            "test_iter:5, nn:3, return_epi_test: -251\n",
            "test_iter:5, nn:4, return_epi_test: -127\n",
            "test_iter:5, nn:5, return_epi_test: -126\n",
            "test_iter:5, nn:6, return_epi_test: -130\n",
            "test_iter:5, nn:7, return_epi_test: -119\n",
            "test_iter:5, nn:8, return_epi_test: -347\n",
            "test_iter:5, nn:9, return_epi_test: -3\n",
            "return_test[5] -160\n",
            "| Reward: -2 | Episode: 125 | Total step num: 25000 |\n",
            "The result of the trial no.2 was saved.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# run parameters\n",
        "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v1}')\n",
        "parser.add_argument('--env-id', type=int, default=0, help='choose the gym env- tested on {Pendulum-v1}')\n",
        "parser.add_argument('--random-seed', help='random seed for repeatability', default=1, type=int)\n",
        "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=1001, type=int)\n",
        "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000, type=int)\n",
        "parser.add_argument('--trial-num', help='number of trials', default=3, type=int)\n",
        "# parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
        "parser.add_argument('--total-step-num', help='total number of time steps', default=25000, type=int)\n",
        "parser.add_argument('--eval-step-freq', help='frequency of evaluating the policy', default=5000, type=int)\n",
        "parser.add_argument('--test-num', help='number of test episodes', default=10, type=int)\n",
        "\n",
        "parser.add_argument('--result-file', help='file name for storing results from multiple trials',\n",
        "                    default='./results/trials/sac/trials_sac_')\n",
        "parser.add_argument('--trial-idx', help='index of trials', default=0, type=int)\n",
        "parser.add_argument('--monitor-dir', help='directory for recording', default='results/video/sac')\n",
        "parser.add_argument('--model-save-freq', help='frequency of evaluating the policy', default=10000, type=int)\n",
        "parser.add_argument('--model-folder',  default='./model/sac')\n",
        "\n",
        "parser.add_argument(\"--start_timesteps\", default=1e4, type=int)  # How many time steps purely random policy is run for\n",
        "parser.add_argument(\"--expl_noise\", default=0.1, type=float)  # Std of Gaussian exploration noise\n",
        "parser.add_argument(\"--batch_size\", default=256, type=int)  # Batch size for both actor and critic\n",
        "parser.add_argument(\"--update_freq\", default=1, type=int)  # Number of policy updates\n",
        "\n",
        "parser.set_defaults(render_env=False)\n",
        "\n",
        "args_tmp, unknown = parser.parse_known_args()\n",
        "\n",
        "if args_tmp.env is None:\n",
        "    env_dict = {0: \"Pendulum-v1\"}\n",
        "    args_tmp.env = env_dict[args_tmp.env_id]\n",
        "args = vars(args_tmp)\n",
        "\n",
        "return_set = []\n",
        "for ite in range(int(args['trial_num'])):\n",
        "    print('Trial Number:', ite)\n",
        "\n",
        "    index = int(ite) + int(args['trial_idx'])\n",
        "    env = gym.make(args['env'])\n",
        "\n",
        "    np.random.seed(index)\n",
        "    env.seed(index)\n",
        "\n",
        "    env_test = gym.make(args['env'])\n",
        "    env_test.seed(index)\n",
        "\n",
        "    print('action_space.shape', env.action_space.shape)\n",
        "    print('observation_space.shape', env.observation_space.shape)\n",
        "    action_bound = float(env.action_space.high[0])\n",
        "\n",
        "    assert (env.action_space.high[0] == -env.action_space.low[0])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    agent = SAC(state_dim=state_dim, action_dim=action_dim, max_action=action_bound)\n",
        "\n",
        "    step_R_i = train(env, env_test, agent, args, index)\n",
        "    return_set.append(step_R_i)\n",
        "\n",
        "    result_path = \"./results/trials/sac\"\n",
        "    result_filename = args['result_file'] +  \\\n",
        "                      '_update_freq_' + str(int(args['update_freq'])) + '_' + args['env'] +  \\\n",
        "                      '_trial_idx_' + str(index) + '.txt'\n",
        "    try:\n",
        "        import pathlib\n",
        "        pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
        "        np.savetxt(result_filename, np.asarray(step_R_i))\n",
        "        print('The result of the trial no.' + str(index) + ' was saved.')\n",
        "    except:\n",
        "        print(\"A result directory does not exist and cannot be created. The trial results are not saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "figure(figsize=(7, 6))\n",
        "\n",
        "t = np.arange(0, int(args['total_step_num']) + 1, int(args['eval_step_freq'])) * 0.001\n",
        "\n",
        "mean = np.mean(np.asarray(return_set), axis=0)\n",
        "std = np.std(np.asarray(return_set), axis=0)\n",
        "color = 'b'\n",
        "label = 'SAC'\n",
        "plt.plot(t, mean, color, label=label)\n",
        "plt.fill(np.concatenate([t, t[::-1]]), np.concatenate([mean - 1.9600 * std,\n",
        "                                      (mean + 1.9600 * std)[::-1]]), alpha=.1, fc=color, ec='None')\n",
        "\n",
        "plt.xlabel('Time steps [x 1e3]', fontsize=14)\n",
        "plt.ylabel('Return', fontsize=14)\n",
        "plt.legend(loc='lower right', fontsize=14)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
